{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Making the VIDEO reference catalogues\n",
    "\n",
    "\n",
    "Here we will make a video set of reference catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.meas.algorithms.htmIndexer import HtmIndexer\n",
    "from lsst.geom import SpherePoint \n",
    "from lsst.geom import degrees\n",
    "from lsst.afw.image import abMagErrFromFluxErr, abMagFromFlux\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG = ''\n",
    "PS_CATS = \"../dmu0/dmu0_PanStarrs/data/ps1_pv3_3pi_20170110/\"\n",
    "PS_CATS19 = \"../dmu0/dmu0_PanStarrs/data/ps1_pv3_3pi_20170110_GmagLT19/\"\n",
    "EX_CAT = \"../dmu0/dmu0_PanStarrs/data/ps1_pv3_3pi_20170110/149504.fits\"\n",
    "EX_MS = \"../dmu0/dmu0_PanStarrs/data/ps1_pv3_3pi_20170110/master_schema.fits\"\n",
    "MASS_CAT = '../dmu0/dmu0_2MASS/data/fp_2mass.fp_psc_29182.tbl'\n",
    "VIDEO_CAT = '../dmu0/dmu0_VISTA/dmu0_VIDEO/data/vista_video_sxds_vsa.fits.gz'\n",
    "#AB HELP catalogue\n",
    "PRIVATE_VIDEO_CAT = '../dmu0/dmu0_VISTA/dmu0_VIDEO/data/VIDEO-all_2017-02-12_fullcat_errfix_v2_XMM-LSS.fits'\n",
    "#VISTA Vega DR5 catalogue cut to stars\n",
    "DR5_VIDEO_CAT = '../dmu0/dmu0_VISTA/dmu0_VIDEO/data/video_dr5_all_pstar09.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Table.read(EX_CAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getShards(ra, dec, radius):\n",
    "    htm = HtmIndexer(depth=7)\n",
    "    shards, onBoundary = htm.getShardIds(SpherePoint(ra*degrees, dec*degrees), radius*degrees)\n",
    "    return shards\n",
    "s = getShards(35.428,  -4.90777, 3.0)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Everything in SXDS DUD region\n",
    "ps_refcats = getShards(36.,  -5.0, 3.0)\n",
    "files = ''\n",
    "for c in ps_refcats:\n",
    "    files += '{}.fits,'.format(c)\n",
    "print('scp ir-shir1@login.hpc.cam.ac.uk:~/rds/rds-iris-ip005/ras81/lsst-ir-fusion/dmu0/dmu0_PanSTARRS/data/ps1_pv3_3pi_20170110/\\{'\n",
    "      +files[:-1]+'\\} ./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_table(table):\n",
    "    \"\"\"Take a table produced by a VO query and remove all empty columns\n",
    "    \n",
    "    Often many columns are empty and make the tables hard to read.\n",
    "    The function also converts columsn that are objects to strings.\n",
    "    Object columns prevent writing to fits.\n",
    "    \n",
    "    Inputs\n",
    "    =======\n",
    "    table,    Astropy.table.Table\n",
    "        The input table\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    table,    Astropy.table.Table\n",
    "         The modified table.\n",
    "    \n",
    "    \"\"\"\n",
    "    table = table.copy()\n",
    "    if len(table) == 0:\n",
    "        return table\n",
    "    for col in table.colnames:\n",
    "        #Remove empty columns\n",
    "        try:\n",
    "            if np.all(table[col].mask):\n",
    "                print(\"Removing empty column: {}\".format(col))\n",
    "                table.remove_column(col)\n",
    "                continue\n",
    "        except AttributeError:\n",
    "            pass\n",
    "            #print(\"{} is not a masked columns\".format(col))\n",
    "            \n",
    "        #Get rid of column type object from VO queries\n",
    "        if table[col].dtype == 'object':\n",
    "            print(\"Converting column {} type from object to string\".format(col) )\n",
    "            table[col] = table[col].astype(str)\n",
    " \n",
    "        #Get rid of unit '-' from some tables\n",
    "        if table[col].unit == '-':\n",
    "            print(\"Converting column {} unit from '-' to None\".format(col) )\n",
    "            table[col].unit = None   \n",
    "            \n",
    "        #replace masked floats with nans     \n",
    "        if (\n",
    "            (table[col].dtype == float) \n",
    "            or (table[col].dtype == 'float32')\n",
    "            or (table[col].dtype == 'float64')\n",
    "        ):\n",
    "            table[col].fill_value = np.nan\n",
    "    \n",
    "    table = table.filled()\n",
    "            \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion from Vega magnitudes to AB is done using values from \n",
    "# http://casu.ast.cam.ac.uk/surveys-projects/vista/technical/filter-set\n",
    "# additional terms for VISTA to True Vega from Carlos Gonzalez et al.\n",
    "vega_to_ab = {\n",
    "    \"z\":0.502 + 0.004,\n",
    "    \"y\":0.600 - 0.022,\n",
    "    \"j\":0.916 + 0.0,\n",
    "    \"h\":1.366 + 0.019,\n",
    "    \"ks\":1.827 - 0.011,\n",
    "    #\"b118\":0.853\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.read(PRIVATE_VIDEO_CAT).colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bands: Z,Y,J,H,K\n",
    "imported_columns = OrderedDict({\n",
    "        'ID': \"id\",\n",
    "        'ALPHA_J2000': \"ra\",\n",
    "        'DELTA_J2000': \"dec\",\n",
    "        'K_CLASS_STAR':  \"stellarity\",\n",
    "        'Z_MAG_APER_3': \"m_ap_vista_z2\", \n",
    "        'Z_MAGERR_APER_3': \"merr_ap_vista_z2\",\n",
    "        'Y_MAG_APER_3': \"m_ap_vista_y2\", \n",
    "        'Y_MAGERR_APER_3': \"merr_ap_vista_y2\",\n",
    "        'J_MAG_APER_3': \"m_ap_vista_j\", \n",
    "        'J_MAGERR_APER_3': \"merr_ap_vista_j\",        \n",
    "        'H_MAG_APER_3': \"m_ap_vista_h\", \n",
    "        'H_MAGERR_APER_3': \"merr_ap_vista_h\",        \n",
    "        'K_MAG_APER_3': \"m_ap_vista_ks\", \n",
    "        'K_MAGERR_APER_3': \"merr_ap_vista_ks\",\n",
    "    })\n",
    "\n",
    "\n",
    "catalogue = Table.read(PRIVATE_VIDEO_CAT)[list(imported_columns)]\n",
    "for column in imported_columns:\n",
    "    new_col = 'video_' + imported_columns[column]\n",
    "    catalogue[column].name = new_col\n",
    "    \n",
    "    \n",
    "catalogue['video_ra'].unit = u.deg\n",
    "catalogue['video_dec'].unit = u.deg\n",
    "catalogue['video_ra'].convert_unit_to(u.rad)\n",
    "catalogue['video_dec'].convert_unit_to(u.rad)\n",
    "\n",
    "for col in catalogue.colnames:\n",
    "    if col.startswith('video_m_ap'):\n",
    "        print(col)\n",
    "        mask = catalogue[col] <= 0\n",
    "        mask |= catalogue[col] >  30\n",
    "        catalogue[col][mask] = np.nan\n",
    "        catalogue[col.replace('video_m', 'video_merr')][mask] = np.nan\n",
    "        \n",
    "        \n",
    "        # Convert magnitude from Vega to AB\n",
    "        #before = np.nanmean(catalogue[col])\n",
    "        #catalogue[col] += vega_to_ab[col.split('_')[-1].replace('2', '')]\n",
    "        #print(col, vega_to_ab[col.split('_')[-1].replace('2', '')], np.nanmean(catalogue[col])-before)\n",
    "#v_flux = Table.read('../dmu0/dmu0_VISTA/dmu0_VHS/data/VHS_XMM-LSS.fits')\n",
    "v_flux = catalogue[catalogue['video_stellarity'] >= 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(catalogue['video_stellarity'] >= 0.9), len(catalogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_flux[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(v_flux['video_m_ap_vista_ks'], bins=100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(~np.isnan(v_flux['video_m_ap_vista_ks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "for col in v_flux.colnames:\n",
    "    if col.startswith('f'):\n",
    "        v_flux[col] /= 1.E6\n",
    "        v_flux[col].unit = u.Jansky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_flux['video_ra'].unit = u.rad\n",
    "v_flux['video_dec'].unit = u.rad\n",
    "v_flux['video_ra'].convert_unit_to(u.rad)\n",
    "v_flux['video_dec'].convert_unit_to(u.rad)\n",
    "#v_flux['ra'].name = 'v_ra'\n",
    "#v_flux['dec'].name = 'v_dec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#selected = v_flux['f_vista_y'] / v_flux['ferr_vista_y'] > 5\n",
    "#selected &= v_flux['f_vista_ks'] / v_flux['ferr_vista_ks'] > 5\n",
    "plt.style.use('seaborn-notebook')\n",
    "plt.figure(1, figsize=(4, 4), dpi=140)\n",
    "plt.scatter(v_flux['video_m_ap_vista_j'] - v_flux['video_m_ap_vista_ks'],\n",
    "            v_flux['video_m_ap_vista_ks'],\n",
    "            edgecolors='None', s=0.05, c='k', rasterized=True)\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(25, 12)\n",
    "plt.xlabel('$J-Ks$')\n",
    "plt.ylabel('$Ks$')\n",
    "plt.subplots_adjust(left=0.125, bottom=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "from astropy.coordinates import SkyCoord, Angle\n",
    "import astropy.units as u\n",
    "from collections import Counter\n",
    "from astropy.table import Table, Column, hstack, vstack\n",
    "def merge_catalogues(cat_1, cat_2, racol_2, decol_2, radius=0.4*u.arcsec):\n",
    "    \"\"\"Merge two catalogues\n",
    "    This function merges the second catalogue into the first one using the\n",
    "    given radius to associate identical sources.  This function takes care to\n",
    "    associate only one source of one catalogue to the other.  The sources that\n",
    "    may be associated to various counterparts in the other catalogue are\n",
    "    flagged as “maybe spurious association” with a true value in the\n",
    "    flag_merged column.  If this column is present in the first catalogue, it's\n",
    "    content is “inherited” during the merge.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cat_1: astropy.table.Table\n",
    "        The table containing the first catalogue.  This is the master catalogue\n",
    "        used during the merge.  If it has a “flag_merged” column it's content\n",
    "        will be re-used in the flagging of the spurious merges.  This catalogue\n",
    "        must contain a ‘ra’ and a ‘dec’ columns with the position in decimal\n",
    "        degrees.\n",
    "    cat_2: astropy.table.Table\n",
    "        The table containing the second catalogue.\n",
    "    racol_2: string\n",
    "        Name of the column in the second table containing the right ascension\n",
    "        in decimal degrees.\n",
    "    decol_2: string\n",
    "        Name of the column in the second table containing the declination in\n",
    "        decimal degrees.\n",
    "    radius: astropy.units.quantity.Quantity\n",
    "        The radius to associate identical sources in the two catalogues.\n",
    "    Returns\n",
    "    -------\n",
    "    astropy.table.Table\n",
    "        The merged catalogue.\n",
    "    \"\"\"\n",
    "    cat_1 = cat_1.copy()\n",
    "    cat_2 = cat_2.copy()\n",
    "    cat_1['ra'].unit = u.rad\n",
    "    cat_1['dec'].unit = u.rad\n",
    "    coords_1 = SkyCoord(cat_1['ra'], cat_1['dec'])\n",
    "\n",
    "    \n",
    "    cat_2[racol_2].unit = u.rad\n",
    "    cat_2[decol_2].unit = u.rad\n",
    "    coords_2 = SkyCoord(cat_2[racol_2], cat_2[decol_2])\n",
    "\n",
    "    # Search for sources in second catalogue matching the sources in the first\n",
    "    # one.\n",
    "    idx_2, idx_1, d2d, _ = coords_1.search_around_sky(coords_2, radius)\n",
    "\n",
    "    # We want to flag the possible mis-associations, i.e. the sources in each\n",
    "    # catalogue that are associated to several sources in the other one, but\n",
    "    # also all the sources that are associated to a problematic source in the\n",
    "    # other catalogue (e.g. if two sources in the first catalogue are\n",
    "    # associated to the same source in the second catalogue, they must be\n",
    "    # flagged as potentially problematic).\n",
    "    #\n",
    "    # Search for duplicate associations\n",
    "    toflag_idx_1 = np.unique([item for item, count in Counter(idx_1).items()\n",
    "                              if count > 1])\n",
    "    toflag_idx_2 = np.unique([item for item, count in Counter(idx_2).items()\n",
    "                              if count > 1])\n",
    "    # Flagging the sources associated to duplicates\n",
    "    dup_associated_in_idx1 = np.in1d(idx_2, toflag_idx_2)\n",
    "    dup_associated_in_idx2 = np.in1d(idx_1, toflag_idx_1)\n",
    "    toflag_idx_1 = np.unique(np.concatenate(\n",
    "        (toflag_idx_1, idx_1[dup_associated_in_idx1])\n",
    "    ))\n",
    "    toflag_idx_2 = np.unique(np.concatenate(\n",
    "        (toflag_idx_2, idx_2[dup_associated_in_idx2])\n",
    "    ))\n",
    "\n",
    "    # Adding the flags to the catalogue.  In the second catalogue, the column\n",
    "    # is named \"flag_merged_2\" and will be combined to the flag_merged column\n",
    "    # one the merge is done.\n",
    "    try:\n",
    "        cat_1[\"flag_merged\"] |= np.in1d(np.arange(len(cat_1), dtype=int),\n",
    "                                        toflag_idx_1)\n",
    "    except KeyError:\n",
    "        cat_1.add_column(Column(\n",
    "            data=np.in1d(np.arange(len(cat_1), dtype=int), toflag_idx_1),\n",
    "            name=\"flag_merged\"\n",
    "        ))\n",
    "    try:\n",
    "        cat_2[\"flag_merged_2\"] |= np.in1d(np.arange(len(cat_2), dtype=int), toflag_idx_2)\n",
    "    except KeyError:\n",
    "        cat_2.add_column(Column(\n",
    "            data=np.in1d(np.arange(len(cat_2), dtype=int), toflag_idx_2),\n",
    "            name=\"flag_merged_2\"\n",
    "        ))\n",
    "\n",
    "\n",
    "    # Now that we have flagged the maybe spurious associations, we want to\n",
    "    # associate each source of each catalogue to at most one source in the\n",
    "    # other one.\n",
    "\n",
    "    # We sort the indices by the distance to take the nearest counterparts in\n",
    "    # the following steps.\n",
    "    sort_idx = np.argsort(d2d)\n",
    "    idx_1 = idx_1[sort_idx]\n",
    "    idx_2 = idx_2[sort_idx]\n",
    "\n",
    "    # These array will contain the indexes of the matching sources in both\n",
    "    # catalogues.\n",
    "    match_idx_1 = np.array([], dtype=int)\n",
    "    match_idx_2 = np.array([], dtype=int)\n",
    "\n",
    "    while len(idx_1) > 0:\n",
    "\n",
    "        both_first_idx = np.sort(np.intersect1d(\n",
    "            np.unique(idx_1, return_index=True)[1],\n",
    "            np.unique(idx_2, return_index=True)[1],\n",
    "        ))\n",
    "\n",
    "        new_match_idx_1 = idx_1[both_first_idx]\n",
    "        new_match_idx_2 = idx_2[both_first_idx]\n",
    "\n",
    "        match_idx_1 = np.concatenate((match_idx_1, new_match_idx_1))\n",
    "        match_idx_2 = np.concatenate((match_idx_2, new_match_idx_2))\n",
    "\n",
    "        # We remove the matching sources in both catalogues.\n",
    "        to_remove = (np.in1d(idx_1, new_match_idx_1) |\n",
    "                     np.in1d(idx_2, new_match_idx_2))\n",
    "        idx_1 = idx_1[~to_remove]\n",
    "        idx_2 = idx_2[~to_remove]\n",
    "\n",
    "    # Indices of un-associated object in both catalogues.\n",
    "    unmatched_idx_1 = np.delete(np.arange(len(cat_1), dtype=int),match_idx_1)\n",
    "    unmatched_idx_2 = np.delete(np.arange(len(cat_2), dtype=int),match_idx_2)\n",
    "\n",
    "    # Sources only in cat_1\n",
    "    only_in_cat_1 = cat_1[unmatched_idx_1]\n",
    "\n",
    "    # Sources only in cat_2\n",
    "    only_in_cat_2 = cat_2[unmatched_idx_2]\n",
    "    # We are using the ra and dec columns from cat_2 for the position.\n",
    "    only_in_cat_2[racol_2].name = \"ra\"\n",
    "    only_in_cat_2[decol_2].name = \"dec\"\n",
    "\n",
    "    # Merged table of sources in both catalogues.\n",
    "    both_in_cat_1_and_cat_2 = hstack([cat_1[match_idx_1], cat_2[match_idx_2]])\n",
    "    # We don't need the positions from the second catalogue anymore.\n",
    "    both_in_cat_1_and_cat_2.remove_columns([racol_2, decol_2])\n",
    "\n",
    "    # Logging the number of rows\n",
    "    LOGGER.info(\"There are %s sources only in the first catalogue\",\n",
    "                len(only_in_cat_1))\n",
    "    LOGGER.info(\"There are %s sources only in the second catalogue\",\n",
    "                len(only_in_cat_2))\n",
    "    LOGGER.info(\"There are %s sources in both catalogues\",\n",
    "                len(both_in_cat_1_and_cat_2))\n",
    "\n",
    "    merged_catalogue = vstack([only_in_cat_1, both_in_cat_1_and_cat_2,\n",
    "                               only_in_cat_2])\n",
    "\n",
    "    # When vertically stacking the catalogues, some values in the flag columns\n",
    "    # are masked because they did not exist in the catalogue some row originate\n",
    "    # from. We must set them to the appropriate value.\n",
    "    for colname in merged_catalogue.colnames:\n",
    "        if 'flag' in colname:\n",
    "            merged_catalogue[colname][merged_catalogue[colname].mask] = False\n",
    "\n",
    "    # We combined the flag_merged flags\n",
    "    merged_catalogue['flag_merged'] |= merged_catalogue['flag_merged_2']\n",
    "    merged_catalogue.remove_column('flag_merged_2')\n",
    "    merged_catalogue.remove_column('flag_merged')\n",
    "    return merged_catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_cat = Table.read(EX_CAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data/ref_cats_video/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIDEO REF CAT\n",
    "for c in ps_refcats:\n",
    "    r_cat = Table.read(\"../dmu0/dmu0_PanStarrs/data/ps1_pv3_3pi_20170110/{}.fits\".format(c))\n",
    "    r_cat['coord_ra'].name = 'ra'\n",
    "    r_cat['coord_dec'].name = 'dec'\n",
    "    merge = merge_catalogues(r_cat, v_flux, 'video_ra', 'video_dec', radius=0.8*u.arcsec)\n",
    "    merge=clean_table(merge)\n",
    "    has_both  = (\n",
    "        (merge['g_flux'] >0.)\n",
    "        # &(merge['video_m_ap_vista_ks']>0.) #Take all panstarrs objects\n",
    "    )\n",
    "\n",
    "    merge = merge[has_both]\n",
    "    \n",
    "    if np.sum(~np.isnan(merge['video_m_ap_vista_ks'])) == 0:\n",
    "        #print('No VIDEO objects in {}'.format(c))\n",
    "        continue\n",
    "    \n",
    "    merge['ra'].name = 'coord_ra'\n",
    "    merge['dec'].name = 'coord_dec'\n",
    "    #convert rad to degs?\n",
    "    merge['coord_ra'] = (180./np.pi)*merge['coord_ra']\n",
    "    merge['coord_ra'].unit = u.deg\n",
    "    merge['coord_dec'] = (180./np.pi)*merge['coord_dec']\n",
    "    merge['coord_dec'].unit = u.deg\n",
    "    merge['pm_ra_err'].name = 'pm_raErr'\n",
    "    merge['pm_dec_err'].name = 'pm_decErr'\n",
    "    try:\n",
    "        merge['coord_raErr'] = (180./np.pi)*merge['coord_ra_err']\n",
    "        merge['coord_raErr'].unit = u.deg\n",
    "        merge['coord_decErr'] = (180./np.pi)*merge['coord_dec_err']\n",
    "        merge['coord_decErr'].unit = u.deg\n",
    "        merge.remove_columns['coord_ra_err', 'coord_dec_err']\n",
    "    except:\n",
    "        merge['coord_raErr'] = (180./np.pi)*merge['coord_raErr']\n",
    "        merge['coord_raErr'].unit = u.deg\n",
    "        merge['coord_decErr'] = (180./np.pi)*merge['coord_decErr']\n",
    "        merge['coord_decErr'].unit = u.deg\n",
    "    #flags col only in James Mulaney 19 cut cat\n",
    "    #merge['flags'].format = '1X'\n",
    "    merge['epoch'] = merge['epoch'].astype('int32')\n",
    "    #merge[has_both].write('./data/refcats/{}.fits'.format(c), overwrite=True)\n",
    "    \n",
    "    for col in merge.colnames:\n",
    "        #go through every flux column and convert to mags and rename\n",
    "        if col.endswith('_flux'):\n",
    "            err_col = col + 'Sigma'\n",
    "            mag = abMagFromFlux(merge[col])\n",
    "            mag_err = abMagErrFromFluxErr(merge[err_col],merge[col])\n",
    "            merge[col] = mag\n",
    "            merge[col].name = col[:-5]\n",
    "            merge[err_col] = mag_err\n",
    "            merge[err_col].name = err_col.replace('fluxSigma','err')\n",
    "        #Go through every video mag col and rename\n",
    "        if col.startswith('video_m_'):\n",
    "            #Leave just band and replace z/y with z2/y2\n",
    "            merge[col].name = col.split('_')[-1]\n",
    "           \n",
    "            merge[col.replace('video_m_', 'video_merr_')].name =  col.split('_')[-1] + '_err'\n",
    "    merge.remove_columns(['video_stellarity', 'video_id'])\n",
    "    merge.write('./data/ref_cats_video/{}.fits'.format(c), overwrite=True)\n",
    "    ex=merge.copy()\n",
    "    print(\"Shard {} has {} objects and {} VIDEO mags.\".format(c, len(merge), np.sum(~np.isnan(merge['ks']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsst",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
