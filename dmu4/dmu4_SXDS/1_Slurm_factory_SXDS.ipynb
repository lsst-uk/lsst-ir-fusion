{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make slurm files required to produce SXDS joint VISTA-HSC data product.\n",
    "\n",
    "In this notebook we will make all the slurm files required to run the whole VISTA-VIDEO HSC-DUD joint photometry pipeline.\n",
    "\n",
    "We need to find all the patches in the HSC imaging and produce a slurm pipeline file for every patch or group of patches.\n",
    "\n",
    "This will be a maximum of around 4 tracts * 91 patches per tract = 364 patches\n",
    "\n",
    "We will also need to set up the data directories including linking relevant reference catalogues and copying the required HSC data products which are already processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Find all the relevant VIDEO images.\n",
    "\n",
    "The first stage is parallesised by ccd. We will create one job for every date. This should be small enough to fit in a 24hr job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = ['20121122', '20171027']\n",
    "\n",
    "sxds_patches = [8282,8283,8284,8523,8524,8525,8765,8766,8767] #manually got these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For simplicity lets ingest all the images (They are only links and this stage is fast)\n",
    "!mkdir data\n",
    "!mkdir slurm\n",
    "for date in date_list:\n",
    "    #!ingestImages.py data /path/to/vista/{date}/*[0-9].fit #Exposures\n",
    "    !ingestImages.py data /path/to/vista/{date}/*_st.fit #Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = [date[0:4]+'-'+date[4:6]+'-'+date[6:9] for date in date_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Process CCDs\n",
    "\n",
    "This stage is parallelised accroding to the raw files ingested. We are going to make one job per date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!/bin/bash\n",
      "source loadLSST.bash\n",
      "setup lsst_distrib\n",
      "setup obs_vista\n",
      "processCcd.py data --rerun processCcdOutputs --id obsDate=2012-11-22\n",
      "    \n",
      "./slurm/processCcd_{}.sh\n",
      "./slurm/processCcd_{}.slurm\n",
      "\n",
      "!/bin/bash\n",
      "source loadLSST.bash\n",
      "setup lsst_distrib\n",
      "setup obs_vista\n",
      "processCcd.py data --rerun processCcdOutputs --id obsDate=2017-10-27\n",
      "    \n",
      "./slurm/processCcd_{}.sh\n",
      "./slurm/processCcd_{}.slurm\n"
     ]
    }
   ],
   "source": [
    "proc_template = \"\"\"\n",
    "!/bin/bash\n",
    "source /rfs/project/rfs-L33A9wsNuJk/shared/lsst_stack/loadLSST.bash\n",
    "setup lsst_distrib\n",
    "setup obs_vista\n",
    "processCcd.py data --rerun processCcdOutputs --id obsDate={obsDate}\n",
    "\"\"\"\n",
    "for date in date_list:\n",
    "    \n",
    "    print(proc_template.format(obsDate=date))\n",
    "    print(\"./slurm/processCcd_{}.sh\")\n",
    "    print(\"./slurm/processCcd_{}.slurm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now submit these after the processCcd has run with\n",
    "#qsub ./slurm/processCcd*.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Run full patch\n",
    "Make one shell script and slurm script for each patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HSC preprocessed files must be copied into place\n",
    "#!cp /Users/rs548/GitHub/lsst-ir-fusion/dmu0/dmu0_HSC/data/hsc-release.mtk.nao.ac.jp/archive/filetree/pdr2_dud/deepCoadd-results/HSC-R data/rerun/coaddPhot/deepCoadd-results/HSC-R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_sh = \"\"\"\n",
    "#!/bin/bash\n",
    "makeCoaddTempExp.py data --rerun coadd --selectId filter=VISTA-Y --id filter=VISTA-Y tract={tract} patch={patches} \n",
    "makeCoaddTempExp.py data --rerun coadd --selectId filter=VISTA-Ks --id filter=VISTA-Ks tract={tract} patch={patches} \n",
    "\n",
    "assembleCoadd.py data --rerun coadd --selectId filter=VISTA-Y --id filter=VISTA-Y tract={tract} patch={patches}\n",
    "assembleCoadd.py data --rerun coadd --selectId filter=VISTA-Ks --id filter=VISTA-Ks tract={tract} patch={patches}\n",
    "\n",
    "detectCoaddSources.py data --rerun coadd:coaddPhot --id filter=VISTA-Y tract={tract} patch={patches}\n",
    "detectCoaddSources.py data --rerun coadd:coaddPhot --id filter=VISTA-Ks tract={tract} patch={patches}\n",
    "\n",
    "mergeCoaddDetections.py data --rerun coaddPhot --id filter=VISTA-Y^VISTA-Ks^HSC-R tract={tract} patch={patches}\n",
    "\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=VISTA-Y tract={tract} patch={patches}\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=VISTA-Ks tract={tract} patch={patches}\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=HSC-R tract={tract} patch={patches}\n",
    "\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=VISTA-Y tract={tract} patch={patches}\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=VISTA-Ks tract={tract} patch={patches}\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=HSC-R tract={tract} patch={patches}\n",
    "\n",
    "mergeCoaddMeasurements.py data --rerun coaddPhot --id filter=VISTA-Y^VISTA-Ks^HSC-R tract={tract} patch={patches}\n",
    "\n",
    "forcedPhotCoadd.py data --rerun coaddPhot:coaddForcedPhot --id filter=VISTA-Y tract={tract} patch={patches}\n",
    "forcedPhotCoadd.py data --rerun coaddForcedPhot --id filter=VISTA-Ks tract={tract} patch={patches}\n",
    "forcedPhotCoadd.py data --rerun coaddForcedPhot --id filter=HSC-R tract={tract} patch={patches}\n",
    "\n",
    "\"\"\"\n",
    "template_slurm = \"\"\"\n",
    "#!/bin/bash\n",
    "#!\n",
    "#! Example SLURM job script for Peta4-Skylake (Skylake CPUs, OPA)\n",
    "#! Last updated: Mon 13 Nov 12:25:17 GMT 2017\n",
    "#!\n",
    "\n",
    "#!#############################################################\n",
    "#!#### Modify the options in this section as appropriate ######\n",
    "#!#############################################################\n",
    "\n",
    "#! sbatch directives begin here ###############################\n",
    "#! Name of the job:\n",
    "#SBATCH -J {jobName}\n",
    "#! Which project should be charged:\n",
    "#SBATCH -A IRIS-IP005-CPU\n",
    "#! How many whole nodes should be allocated?\n",
    "#SBATCH --nodes=1\n",
    "#! How many (MPI) tasks will there be in total? (<= nodes*32)\n",
    "#! The skylake/skylake-himem nodes have 32 CPUs (cores) each.\n",
    "#SBATCH --ntasks=1\n",
    "#! How much wallclock time will be required?\n",
    "#SBATCH --time=36:00:00\n",
    "#! What types of email messages do you wish to receive?\n",
    "#SBATCH --mail-type=FAIL\n",
    "#! Uncomment this to prevent the job from being requeued (e.g. if\n",
    "#! interrupted by node failure or system downtime):\n",
    "##SBATCH --no-requeue\n",
    "\n",
    "#! For 6GB per CPU, set \"-p skylake\"; for 12GB per CPU, set \"-p skylake-himem\": \n",
    "#SBATCH -p skylake\n",
    "\n",
    "#! sbatch directives end here (put any additional directives above this line)\n",
    "\n",
    "#! Notes:\n",
    "#! Charging is determined by core number*walltime.\n",
    "#! The --ntasks value refers to the number of tasks to be launched by SLURM only. This\n",
    "#! usually equates to the number of MPI tasks launched. Reduce this from nodes*32 if\n",
    "#! demanded by memory requirements, or if OMP_NUM_THREADS>1.\n",
    "#! Each task is allocated 1 core by default, and each core is allocated 5980MB (skylake)\n",
    "#! and 12030MB (skylake-himem). If this is insufficient, also specify\n",
    "#! --cpus-per-task and/or --mem (the latter specifies MB per node).\n",
    "\n",
    "#! Number of nodes and tasks per node allocated by SLURM (do not change):\n",
    "numnodes=$SLURM_JOB_NUM_NODES\n",
    "numtasks=$SLURM_NTASKS\n",
    "mpi_tasks_per_node=$(echo \"$SLURM_TASKS_PER_NODE\" | sed -e  's/^\\([0-9][0-9]*\\).*$/\\1/')\n",
    "#! ############################################################\n",
    "#! Modify the settings below to specify the application's environment, location \n",
    "#! and launch method:\n",
    "\n",
    "#! Optionally modify the environment seen by the application\n",
    "#! (note that SLURM reproduces the environment at submission irrespective of ~/.bashrc):\n",
    ". /etc/profile.d/modules.sh                # Leave this line (enables the module command)\n",
    "module purge                               # Removes all modules still loaded\n",
    "module load rhel7/default-peta4            # REQUIRED - loads the basic environment\n",
    "\n",
    "#! Insert additional module load commands after this line if needed:\n",
    "\n",
    "#! Full path to application executable: \n",
    "application=\"{shellScript}\"\n",
    "\n",
    "#! Run options for the application:\n",
    "options=\"\"\n",
    "\n",
    "#! Work directory (i.e. where the job will run):\n",
    "workdir=\"$SLURM_SUBMIT_DIR\"  # The value of SLURM_SUBMIT_DIR sets workdir to the directory\n",
    "                             # in which sbatch is run.\n",
    "\n",
    "#! Are you using OpenMP (NB this is unrelated to OpenMPI)? If so increase this\n",
    "#! safe value to no more than 32:\n",
    "export OMP_NUM_THREADS=1\n",
    "\n",
    "#! Number of MPI tasks to be started by the application per node and in total (do not change):\n",
    "np=$[${numnodes}*${mpi_tasks_per_node}]\n",
    "\n",
    "#! The following variables define a sensible pinning strategy for Intel MPI tasks -\n",
    "#! this should be suitable for both pure MPI and hybrid MPI/OpenMP jobs:\n",
    "export I_MPI_PIN_DOMAIN=omp:compact # Domains are $OMP_NUM_THREADS cores in size\n",
    "export I_MPI_PIN_ORDER=scatter # Adjacent domains have minimal sharing of caches/sockets\n",
    "#! Notes:\n",
    "#! 1. These variables influence Intel MPI only.\n",
    "#! 2. Domains are non-overlapping sets of cores which map 1-1 to MPI tasks.\n",
    "#! 3. I_MPI_PIN_PROCESSOR_LIST is ignored if I_MPI_PIN_DOMAIN is set.\n",
    "#! 4. If MPI tasks perform better when sharing caches/sockets, try I_MPI_PIN_ORDER=compact.\n",
    "\n",
    "\n",
    "#! Uncomment one choice for CMD below (add mpirun/mpiexec options if necessary):\n",
    "\n",
    "#! Choose this for a MPI code (possibly using OpenMP) using Intel MPI.\n",
    "CMD=\"mpirun -ppn $mpi_tasks_per_node -np $np $application $options\"\n",
    "\n",
    "#! Choose this for a pure shared-memory OpenMP parallel program on a single node:\n",
    "#! (OMP_NUM_THREADS threads will be created):\n",
    "#CMD=\"$application $options\"\n",
    "\n",
    "#! Choose this for a MPI code (possibly using OpenMP) using OpenMPI:\n",
    "#CMD=\"mpirun -npernode $mpi_tasks_per_node -np $np $application $options\"\n",
    "\n",
    "\n",
    "###############################################################\n",
    "### You should not have to change anything below this line ####\n",
    "###############################################################\n",
    "\n",
    "cd $workdir\n",
    "echo -e \"Changed directory to `pwd`.\\n\"\n",
    "\n",
    "JOBID=$SLURM_JOB_ID\n",
    "\n",
    "echo -e \"JobID: $JOBID\\n======\"\n",
    "echo \"Time: `date`\"\n",
    "echo \"Running on master node: `hostname`\"\n",
    "echo \"Current directory: `pwd`\"\n",
    "\n",
    "if [ \"$SLURM_JOB_NODELIST\" ]; then\n",
    "        #! Create a machine file:\n",
    "        export NODEFILE=`generate_pbs_nodefile`\n",
    "        cat $NODEFILE | uniq > machine.file.$JOBID\n",
    "        echo -e \"\\nNodes allocated:\\n================\"\n",
    "        echo `cat machine.file.$JOBID | sed -e 's/\\..*$//g'`\n",
    "fi\n",
    "\n",
    "echo -e \"\\nnumtasks=$numtasks, numnodes=$numnodes, mpi_tasks_per_node=$mpi_tasks_per_node (OMP_NUM_THREADS=$OMP_NUM_THREADS)\"\n",
    "\n",
    "echo -e \"\\nExecuting command:\\n==================\\n$CMD\\n\"\n",
    "\n",
    "eval $CMD \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tract in sxds_tracts:\n",
    "    pFolders = glob.glob(HSC_LOC + '/hsc-release.mtk.nao.ac.jp/archive/filetree/pdr2_dud/deepCoadd-results/*/{}/*/calexp*.fits'.format(tract))\n",
    "\n",
    "for tract, patch in sxds_patches:\n",
    "    #write shell script\n",
    "    \"./slurm/patch_{}_{}.sh\".format(tract,patch)\n",
    "    #write slurm script\n",
    "    \"./slurm/patch_{}_{}.slurm\".format(tract,patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now submit these after the processCcd has run with\n",
    "#qsub ./slurm/patch*.slurm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
