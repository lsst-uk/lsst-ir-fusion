{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make slurm files required to produce SXDS joint VISTA-HSC data product.\n",
    "\n",
    "In this notebook we will make all the slurm files required to run the whole VISTA-VIDEO HSC-DUD joint photometry pipeline.\n",
    "\n",
    "We need to find all the patches in the HSC imaging and produce a slurm pipeline file for every patch or group of patches.\n",
    "\n",
    "This will be a maximum of around 4 tracts * 91 patches per tract = 364 patches\n",
    "\n",
    "We will also need to set up the data directories including linking relevant reference catalogues and copying the required HSC data products which are already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table, Column\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSC_LOC = '../../dmu0/dmu0_HSC/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Find all the relevant VIDEO images.\n",
    "\n",
    "The first stage is parallesised by ccd. We will create one job for every date. This should be small enough to fit in a 24hr job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ims = Table.read('../../dmu1/data/video_images_overview_20200820.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ims.add_column(Column(\n",
    "    data= [t.split('/')[-2] for t in video_ims['file']],\n",
    "    name='date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileToType(filename):\n",
    "    filetype = ''\n",
    "    types = {\n",
    "        'tile':'_tl.fit',\n",
    "        'stack':'_st.fit',\n",
    "    }\n",
    "    for k,v in types.items():\n",
    "        #print(k,v)\n",
    "        if filename.endswith(v):\n",
    "            filetype = k\n",
    "       \n",
    "    return filetype\n",
    "video_ims['type'] = [fileToType(f) for f in video_ims['file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rs548/GitHub/lsst_stack/conda/miniconda3-py37_4.8.2/envs/lsst-scipipe-1a1d771/lib/python3.7/site-packages/astropy/table/column.py:1020: RuntimeWarning: invalid value encountered in greater\n",
      "  result = getattr(super(), op)(other)\n",
      "/Users/rs548/GitHub/lsst_stack/conda/miniconda3-py37_4.8.2/envs/lsst-scipipe-1a1d771/lib/python3.7/site-packages/astropy/table/column.py:1020: RuntimeWarning: invalid value encountered in less\n",
      "  result = getattr(super(), op)(other)\n"
     ]
    }
   ],
   "source": [
    "#TODO make more sophisticated overlap tester. Use patches?\n",
    "#make list of patches for every tile?\n",
    "near_sxds = video_ims['type'] == 'tile'\n",
    "near_sxds &= video_ims['ra'] > 32 #generous bounding bos for simplicity \n",
    "near_sxds &= video_ims['ra'] < 39\n",
    "near_sxds &= video_ims['dec'] > -8\n",
    "near_sxds &= video_ims['dec'] < -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "863"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(near_sxds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_list = ['20121122', '20171027'] #test dates\n",
    "date_list = np.unique(video_ims['date'][near_sxds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sxds_tracts = [8282,8283,8284,8523,8524,8525,8765,8766,8767] #manually got these from HSC DR2 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For simplicity lets ingest all the images (They are only links and this stage is fast)\n",
    "#!mkdir data\n",
    "#!mkdir slurm\n",
    "#for date in date_list:\n",
    "#    #!ingestImages.py data /path/to/vista/{date}/*[0-9].fit #Exposures\n",
    "#    !ingestImages.py data /path/to/vista/{date}/*_st.fit #Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = [date[0:4]+'-'+date[4:6]+'-'+date[6:9] for date in date_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Process CCDs\n",
    "\n",
    "This stage is parallelised accroding to the raw files ingested. We are going to make one job per date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_template= \"\"\"\n",
    "#!/bin/bash\n",
    "#!\n",
    "#! Example SLURM job script for Peta4-Skylake (Skylake CPUs, OPA)\n",
    "#! Last updated: Mon 13 Nov 12:25:17 GMT 2017\n",
    "#!\n",
    "\n",
    "#!#############################################################\n",
    "#!#### Modify the options in this section as appropriate ######\n",
    "#!#############################################################\n",
    "\n",
    "#! sbatch directives begin here ###############################\n",
    "#! Name of the job:\n",
    "#SBATCH -J {job_name}\n",
    "#! Which project should be charged:\n",
    "#SBATCH -A IRIS-IP005-CPU\n",
    "#! How many whole nodes should be allocated?\n",
    "#SBATCH --nodes=1\n",
    "#! How many (MPI) tasks will there be in total? (<= nodes*32)\n",
    "#! The skylake/skylake-himem nodes have 32 CPUs (cores) each.\n",
    "#SBATCH --ntasks=1\n",
    "#! How much wallclock time will be required?\n",
    "#SBATCH --time=36:00:00\n",
    "#! What types of email messages do you wish to receive?\n",
    "#SBATCH --mail-type=FAIL\n",
    "#! Uncomment this to prevent the job from being requeued (e.g. if\n",
    "#! interrupted by node failure or system downtime):\n",
    "##SBATCH --no-requeue\n",
    "\n",
    "#! For 6GB per CPU, set \"-p skylake\"; for 12GB per CPU, set \"-p skylake-himem\": \n",
    "#SBATCH -p skylake\n",
    "\n",
    "#! sbatch directives end here (put any additional directives above this line)\n",
    "\n",
    "#! Notes:\n",
    "#! Charging is determined by core number*walltime.\n",
    "#! The --ntasks value refers to the number of tasks to be launched by SLURM only. This\n",
    "#! usually equates to the number of MPI tasks launched. Reduce this from nodes*32 if\n",
    "#! demanded by memory requirements, or if OMP_NUM_THREADS>1.\n",
    "#! Each task is allocated 1 core by default, and each core is allocated 5980MB (skylake)\n",
    "#! and 12030MB (skylake-himem). If this is insufficient, also specify\n",
    "#! --cpus-per-task and/or --mem (the latter specifies MB per node).\n",
    "\n",
    "#! Number of nodes and tasks per node allocated by SLURM (do not change):\n",
    "numnodes=$SLURM_JOB_NUM_NODES\n",
    "numtasks=$SLURM_NTASKS\n",
    "mpi_tasks_per_node=$(echo \"$SLURM_TASKS_PER_NODE\" | sed -e  's/^\\([0-9][0-9]*\\).*$/\\1/')\n",
    "#! ############################################################\n",
    "#! Modify the settings below to specify the application's environment, location \n",
    "#! and launch method:\n",
    "\n",
    "#! Optionally modify the environment seen by the application\n",
    "#! (note that SLURM reproduces the environment at submission irrespective of ~/.bashrc):\n",
    ". /etc/profile.d/modules.sh                # Leave this line (enables the module command)\n",
    "module purge                               # Removes all modules still loaded\n",
    "module load rhel7/default-peta4            # REQUIRED - loads the basic environment\n",
    "\n",
    "#! Insert additional module load commands after this line if needed:\n",
    "\n",
    "#! Full path to application executable: \n",
    "application=\"{sh_name}\"\n",
    "\n",
    "#! Run options for the application:\n",
    "options=\"\"\n",
    "\n",
    "#! Work directory (i.e. where the job will run):\n",
    "workdir=\"$SLURM_SUBMIT_DIR\"  # The value of SLURM_SUBMIT_DIR sets workdir to the directory\n",
    "                             # in which sbatch is run.\n",
    "\n",
    "#! Are you using OpenMP (NB this is unrelated to OpenMPI)? If so increase this\n",
    "#! safe value to no more than 32:\n",
    "export OMP_NUM_THREADS=1\n",
    "\n",
    "#! Number of MPI tasks to be started by the application per node and in total (do not change):\n",
    "np=$[${{numnodes}}*${{mpi_tasks_per_node}}]\n",
    "\n",
    "#! The following variables define a sensible pinning strategy for Intel MPI tasks -\n",
    "#! this should be suitable for both pure MPI and hybrid MPI/OpenMP jobs:\n",
    "export I_MPI_PIN_DOMAIN=omp:compact # Domains are $OMP_NUM_THREADS cores in size\n",
    "export I_MPI_PIN_ORDER=scatter # Adjacent domains have minimal sharing of caches/sockets\n",
    "#! Notes:\n",
    "#! 1. These variables influence Intel MPI only.\n",
    "#! 2. Domains are non-overlapping sets of cores which map 1-1 to MPI tasks.\n",
    "#! 3. I_MPI_PIN_PROCESSOR_LIST is ignored if I_MPI_PIN_DOMAIN is set.\n",
    "#! 4. If MPI tasks perform better when sharing caches/sockets, try I_MPI_PIN_ORDER=compact.\n",
    "\n",
    "\n",
    "#! Uncomment one choice for CMD below (add mpirun/mpiexec options if necessary):\n",
    "\n",
    "#! Choose this for a MPI code (possibly using OpenMP) using Intel MPI.\n",
    "CMD=\"mpirun -ppn $mpi_tasks_per_node -np $np $application $options\"\n",
    "\n",
    "#! Choose this for a pure shared-memory OpenMP parallel program on a single node:\n",
    "#! (OMP_NUM_THREADS threads will be created):\n",
    "#CMD=\"$application $options\"\n",
    "\n",
    "#! Choose this for a MPI code (possibly using OpenMP) using OpenMPI:\n",
    "#CMD=\"mpirun -npernode $mpi_tasks_per_node -np $np $application $options\"\n",
    "\n",
    "\n",
    "###############################################################\n",
    "### You should not have to change anything below this line ####\n",
    "###############################################################\n",
    "\n",
    "cd $workdir\n",
    "echo -e \"Changed directory to `pwd`.\\n\"\n",
    "\n",
    "JOBID=$SLURM_JOB_ID\n",
    "\n",
    "echo -e \"JobID: $JOBID\\n======\"\n",
    "echo \"Time: `date`\"\n",
    "echo \"Running on master node: `hostname`\"\n",
    "echo \"Current directory: `pwd`\"\n",
    "\n",
    "if [ \"$SLURM_JOB_NODELIST\" ]; then\n",
    "        #! Create a machine file:\n",
    "        export NODEFILE=`generate_pbs_nodefile`\n",
    "        cat $NODEFILE | uniq > machine.file.$JOBID\n",
    "        echo -e \"\\nNodes allocated:\\n================\"\n",
    "        echo `cat machine.file.$JOBID | sed -e 's/\\..*$//g'`\n",
    "fi\n",
    "\n",
    "echo -e \"\\nnumtasks=$numtasks, numnodes=$numnodes, mpi_tasks_per_node=$mpi_tasks_per_node (OMP_NUM_THREADS=$OMP_NUM_THREADS)\"\n",
    "\n",
    "echo -e \"\\nExecuting command:\\n==================\\n$CMD\\n\"\n",
    "\n",
    "eval $CMD \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_template = \"\"\"\n",
    "!/bin/bash\n",
    "source /rfs/project/rfs-L33A9wsNuJk/shared/lsst_stack/loadLSST.bash\n",
    "setup lsst_distrib\n",
    "setup obs_vista\n",
    "processCcd.py ../data --rerun processCcdOutputs --id obsDate={obsDate}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for date in date_list:\n",
    "    sh_name = \"./slurm/processCcd_{}.sh\".format(date)\n",
    "    f_sh = open(sh_name, \"a\")\n",
    "    f_sh.write(sh_template.format(obsDate=date))\n",
    "    f_sh.close()\n",
    "    f_slurm = open(\"./slurm/processCcd_{}.slurm\".format(date), \"a\")\n",
    "    f_slurm.write(slurm_template.format(\n",
    "        job_name=\"processCcd_{}\".format(date),\n",
    "        sh_name=sh_name\n",
    "        \n",
    "    ))\n",
    "    f_slurm.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now submit these after the processCcd has run with\n",
    "#qsub ./slurm/processCcd*.slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Run full patch\n",
    "Make one shell script and slurm script for each patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HSC preprocessed files must be copied into place\n",
    "#!cp /Users/rs548/GitHub/lsst-ir-fusion/dmu0/dmu0_HSC/data/hsc-release.mtk.nao.ac.jp/archive/filetree/pdr2_dud/deepCoadd-results/HSC-R data/rerun/coaddPhot/deepCoadd-results/HSC-R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_sh_template = \"\"\"\n",
    "#!/bin/bash\n",
    "makeCoaddTempExp.py data --rerun coadd --selectId filter=VISTA-Y --id filter=VISTA-Y tract={tract} patch={patches} \n",
    "makeCoaddTempExp.py data --rerun coadd --selectId filter=VISTA-Ks --id filter=VISTA-J tract={tract} patch={patches} \n",
    "makeCoaddTempExp.py data --rerun coadd --selectId filter=VISTA-Y --id filter=VISTA-H tract={tract} patch={patches} \n",
    "makeCoaddTempExp.py data --rerun coadd --selectId filter=VISTA-Ks --id filter=VISTA-Ks tract={tract} patch={patches} \n",
    "\n",
    "assembleCoadd.py data --rerun coadd --selectId filter=VISTA-Y --id filter=VISTA-Y tract={tract} patch={patches}\n",
    "assembleCoadd.py data --rerun coadd --selectId filter=VISTA-Ks --id filter=VISTA-J tract={tract} patch={patches}\n",
    "assembleCoadd.py data --rerun coadd --selectId filter=VISTA-Y --id filter=VISTA-H tract={tract} patch={patches}\n",
    "assembleCoadd.py data --rerun coadd --selectId filter=VISTA-Ks --id filter=VISTA-Ks tract={tract} patch={patches}\n",
    "\n",
    "detectCoaddSources.py data --rerun coadd:coaddPhot --id filter=VISTA-Y tract={tract} patch={patches}\n",
    "detectCoaddSources.py data --rerun coadd:coaddPhot --id filter=VISTA-J tract={tract} patch={patches}\n",
    "detectCoaddSources.py data --rerun coadd:coaddPhot --id filter=VISTA-H tract={tract} patch={patches}\n",
    "detectCoaddSources.py data --rerun coadd:coaddPhot --id filter=VISTA-Ks tract={tract} patch={patches}\n",
    "\n",
    "#HSC files must be copied\n",
    "mergeCoaddDetections.py data --rerun coaddPhot --id filter=VISTA-Y^VISTA-J^VISTA-H^VISTA-Ks^HSC-G^HSC-R^HSC-I^HSC-Z^HSC-Y tract={tract} patch={patches}\n",
    "\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=VISTA-Y tract={tract} patch={patches}\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=VISTA-J tract={tract} patch={patches}\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=VISTA-H tract={tract} patch={patches}\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=VISTA-Ks tract={tract} patch={patches}\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=HSC-G tract={tract} patch={patches}\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=HSC-R tract={tract} patch={patches}\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=HSC-I tract={tract} patch={patches}\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=HSC-Z tract={tract} patch={patches}\n",
    "deblendCoaddSources.py data --rerun coaddPhot --id filter=HSC-Y tract={tract} patch={patches}\n",
    "\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=VISTA-Y tract={tract} patch={patches}\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=VISTA-J tract={tract} patch={patches}\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=VISTA-H tract={tract} patch={patches}\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=VISTA-Ks tract={tract} patch={patches}\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=HSC-G tract={tract} patch={patches}\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=HSC-R tract={tract} patch={patches}\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=HSC-I tract={tract} patch={patches}\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=HSC-Z tract={tract} patch={patches}\n",
    "measureCoaddSources.py data --rerun coaddPhot --id filter=HSC-Y tract={tract} patch={patches}\n",
    "\n",
    "mergeCoaddMeasurements.py data --rerun coaddPhot --id filter=VISTA-Y^VISTA-J^VISTA-H^VISTA-Ks^HSC-G^HSC-R^HSC-I^HSC-Z^HSC-Y tract={tract} patch={patches}\n",
    "\n",
    "forcedPhotCoadd.py data --rerun coaddPhot:coaddForcedPhot --id filter=VISTA-Y tract={tract} patch={patches}\n",
    "forcedPhotCoadd.py data --rerun coaddForcedPhot --id filter=VISTA-Ks tract={tract} patch={patches}\n",
    "forcedPhotCoadd.py data --rerun coaddForcedPhot --id filter=HSC-R tract={tract} patch={patches}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../dmu0/dmu0_HSC/data'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HSC_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../dmu0/dmu0_HSC/data/hsc-release.mtk.nao.ac.jp/archive/filetree/pdr2_dud/deepCoadd-results/HSC-R/8524/3,5/calexp-HSC-R-8524-3,5.fits']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(HSC_LOC + '/hsc-release.mtk.nao.ac.jp/archive/filetree/pdr2_dud/deepCoadd-results/*/8524/*/calexp*.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_patches_list = {}\n",
    "for tract in sxds_tracts:\n",
    "    p_folders = glob.glob(HSC_LOC + '/hsc-release.mtk.nao.ac.jp/archive/filetree/pdr2_dud/deepCoadd-results/*/{}/*/calexp*.fits'.format(tract))\n",
    "    patches={\"0,0\",\"0,1\"}\n",
    "    tract_patches_list[tract] = patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0,0', '0,1'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tract_patches_list[8282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tract, patches in tract_patches_list:\n",
    "    sh_name = \"./slurm/processCcd_{}.sh\".format(date)\n",
    "    f_sh = open(sh_name, \"a\")\n",
    "    f_sh.write(patch_sh_template.format(tract=tract, patches=patches))\n",
    "    f_sh.close()\n",
    "    f_slurm = open(\"./slurm/processCcd_{}.slurm\".format(date), \"a\")\n",
    "    f_slurm.write(slurm_template.format(\n",
    "        job_name=\"processCcd_{}\".format(date),\n",
    "        sh_name=sh_name\n",
    "        \n",
    "    ))\n",
    "    f_slurm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now submit these after the processCcd has run with\n",
    "#qsub ./slurm/patch*.slurm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
